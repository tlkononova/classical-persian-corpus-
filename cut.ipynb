{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class AlphabetDetector:\n",
    "    def __init__(self, no_memory=False):\n",
    "        self.alphabet_letters = defaultdict(dict)\n",
    "        self.no_memory = no_memory\n",
    "\n",
    "    def is_in_alphabet(self, uchr, alphabet):\n",
    "        if self.no_memory:\n",
    "            return alphabet in ud.name(uchr)\n",
    "        try:\n",
    "            return self.alphabet_letters[alphabet][uchr]\n",
    "        except KeyError:\n",
    "            return self.alphabet_letters[alphabet].setdefault(\n",
    "                uchr, alphabet in ud.name(uchr))\n",
    "\n",
    "    def only_alphabet_chars(self, unistr, alphabet):\n",
    "        return all(self.is_in_alphabet(uchr, alphabet)\n",
    "                   for uchr in unistr if uchr.isalpha())\n",
    "\n",
    "    def detect_alphabet(self, unistr):\n",
    "        return set(ud.name(char).split(' ')[0]\n",
    "                   for char in unistr if char.isalpha())\n",
    "\n",
    "    def is_cyrillic(self, unistr):\n",
    "        return True if self.only_alphabet_chars(unistr, 'CYRILLIC') else False\n",
    "\n",
    "    def is_latin(self, unistr):\n",
    "        return True if self.only_alphabet_chars(unistr, 'LATIN') else False\n",
    "\n",
    "    def is_arabic(self, unistr):\n",
    "        return True if self.only_alphabet_chars(unistr, 'ARABIC') else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ad = AlphabetDetector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#states = {(1,'word'), (2,'trans'), (3,'pos')}\n",
    "auto = {}\n",
    "pos = set()\n",
    "#poslist = \n",
    "state = 0\n",
    "word = ['word', 'pos']\n",
    "Vocab = {}\n",
    "\n",
    "with open(\"./rubinchik/rubinchik.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if ad.is_arabic(line.strip()) and state == 0: \n",
    "            state = 1\n",
    "            word[0] = line.strip()\n",
    "            #print(line)\n",
    "        elif line.strip()[0] == '[' and line.strip()[-1] == ']' and state == 1:\n",
    "            state = 2\n",
    "            #print(line)\n",
    "        elif ad.is_cyrillic(line.strip()) and state == 2: \n",
    "            #if line.split('., ')[0].strip(' \\\\n') in poslist:\n",
    "            state = 3\n",
    "            word[1] = line.split('., ')[0].strip(' \\\\n')\n",
    "            pos.add(word[1])\n",
    "            Vocab[word[0]] = word[1]\n",
    "            state = 0   \n",
    "                #print(word[1])\n",
    "        else:\n",
    "            state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pos\n",
    "with open('./rubinchik/rub_wordlist.txt', 'w', encoding='utf-8') as w:\n",
    "    dicSorted = sorted(Vocab.items(), key=lambda tup: tup[0], reverse=False)\n",
    "    for word in dicSorted:\n",
    "        w.write(word[0]+'\\t'+str(word[1])+'\\n')\n",
    "        #i+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52612"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'آرتل': 'сущ', 'آرتیکل': 'сущ', 'آردی': 'прил', 'آرزو\\u200c': 'сущ', 'آردینه': 'прил', 'آرتیستی': 'нар', 'آرزومندی\\u200c': 'сущ', 'آرتیست\\u200cبازی': 'сущ', 'آردالو': 'сущ', 'آرژان': 'сущ', 'آرد\\u200c': 'сущ', 'آرداله\\u200c': 'сущ', 'آرتیست': 'сущ', 'آرزومند\\u200c': 'прил', 'آرتیشو': 'сущ', 'آذرشست': 'сущ', 'آرده\\u200c': 'сущ', 'آردل\\u200c': 'сущ', 'آرتزین': 'прил', 'آرخانگلسک': 'сущ', 'آرتمیا': 'сущ', 'آرداب\\u200c': 'сущ', 'آردهاله': 'сущ', 'آرتروز': 'сущ', 'آرخالق\\u200c': 'сущ', 'آرتریت': 'сущ', 'آردبیز\\u200c': 'сущ', 'آردن\\u200c': 'сущ', 'آردواز': 'сущ', 'آرژانتین': 'сущ'}\n"
     ]
    }
   ],
   "source": [
    "#print(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ОНВ آزار‌ [āzār]\n",
    "#ОНВ آزما(ی) [āzmā(y)]\n",
    "#гл. ОНВ آژن [āžan]\n",
    "#ОНВ гл. آشکوخ [āškux] и اشکوخ [aškux]\n",
    "#1) ОНВ гл. آفریدن"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "Parent module '' not loaded, cannot perform relative import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bade5fc600f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefault_verbs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizerI\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: Parent module '' not loaded, cannot perform relative import"
     ]
    }
   ],
   "source": [
    "#from __future__ import unicode_literals\n",
    "#import hazm\n",
    "#normalizer = Normalizer()\n",
    "#normalizer.normalize('اصلاح نويسه ها و استفاده از نیم‌فاصله پردازش را آسان مي كند')\n",
    "\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import re, codecs\n",
    "from .utils import default_verbs\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "\n",
    "\n",
    "class WordTokenizer(TokenizerI):\n",
    "\t\"\"\"\n",
    "\t>>> tokenizer = WordTokenizer()\n",
    "\t>>> tokenizer.tokenize('این جمله (خیلی) پیچیده نیست!!!')\n",
    "\t['این', 'جمله', '(', 'خیلی', ')', 'پیچیده', 'نیست', '!!!']\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, verbs_file=default_verbs, join_verb_parts=True):\n",
    "\t\tself._join_verb_parts = join_verb_parts\n",
    "\t\tself.pattern = re.compile(r'([؟!\\?]+|[:\\.،؛»\\]\\)\\}\"«\\[\\(\\{])')\n",
    "\n",
    "\t\tif join_verb_parts:\n",
    "\t\t\tself.after_verbs = set([\n",
    "\t\t\t\t'ام', 'ای', 'است', 'ایم', 'اید', 'اند', 'بودم', 'بودی', 'بود', 'بودیم', 'بودید', 'بودند', 'باشم', 'باشی', 'باشد', 'باشیم', 'باشید', 'باشند',\n",
    "\t\t\t\t'شده_ام', 'شده_ای', 'شده_است', 'شده_ایم', 'شده_اید', 'شده_اند', 'شده_بودم', 'شده_بودی', 'شده_بود', 'شده_بودیم', 'شده_بودید', 'شده_بودند', 'شده_باشم', 'شده_باشی', 'شده_باشد', 'شده_باشیم', 'شده_باشید', 'شده_باشند',\n",
    "\t\t\t\t'نشده_ام', 'نشده_ای', 'نشده_است', 'نشده_ایم', 'نشده_اید', 'نشده_اند', 'نشده_بودم', 'نشده_بودی', 'نشده_بود', 'نشده_بودیم', 'نشده_بودید', 'نشده_بودند', 'نشده_باشم', 'نشده_باشی', 'نشده_باشد', 'نشده_باشیم', 'نشده_باشید', 'نشده_باشند',\n",
    "\t\t\t\t'شوم', 'شوی', 'شود', 'شویم', 'شوید', 'شوند', 'شدم', 'شدی', 'شد', 'شدیم', 'شدید', 'شدند',\n",
    "\t\t\t\t'نشوم', 'نشوی', 'نشود', 'نشویم', 'نشوید', 'نشوند', 'نشدم', 'نشدی', 'نشد', 'نشدیم', 'نشدید', 'نشدند',\n",
    "\t\t\t\t'می‌شوم', 'می‌شوی', 'می‌شود', 'می‌شویم', 'می‌شوید', 'می‌شوند', 'می‌شدم', 'می‌شدی', 'می‌شد', 'می‌شدیم', 'می‌شدید', 'می‌شدند',\n",
    "\t\t\t\t'نمی‌شوم', 'نمی‌شوی', 'نمی‌شود', 'نمی‌شویم', 'نمی‌شوید', 'نمی‌شوند', 'نمی‌شدم', 'نمی‌شدی', 'نمی‌شد', 'نمی‌شدیم', 'نمی‌شدید', 'نمی‌شدند',\n",
    "\t\t\t\t'خواهم_شد', 'خواهی_شد', 'خواهد_شد', 'خواهیم_شد', 'خواهید_شد', 'خواهند_شد',\n",
    "\t\t\t\t'نخواهم_شد', 'نخواهی_شد', 'نخواهد_شد', 'نخواهیم_شد', 'نخواهید_شد', 'نخواهند_شد',\n",
    "\t\t\t])\n",
    "\n",
    "\t\t\tself.before_verbs = set([\n",
    "\t\t\t\t'خواهم', 'خواهی', 'خواهد', 'خواهیم', 'خواهید', 'خواهند',\n",
    "\t\t\t\t'نخواهم', 'نخواهی', 'نخواهد', 'نخواهیم', 'نخواهید', 'نخواهند'\n",
    "\t\t\t])\n",
    "\n",
    "\t\t\twith codecs.open(verbs_file, encoding='utf8') as verbs_file:\n",
    "\t\t\t\tself.verbs = list(reversed([verb.strip() for verb in verbs_file if verb]))\n",
    "\t\t\t\tself.bons = set([verb.split('#')[0] for verb in self.verbs])\n",
    "\t\t\t\tself.verbe = set([bon +'ه' for bon in self.bons] + ['ن'+ bon +'ه' for bon in self.bons])\n",
    "\n",
    "\tdef tokenize(self, text):\n",
    "\t\ttext = self.pattern.sub(r' \\1 ', text.replace('\\n', ' ').replace('\\t', ' '))\n",
    "\t\ttokens = [word for word in text.split(' ') if word]\n",
    "\t\tif self._join_verb_parts:\n",
    "\t\t\ttokens = self.join_verb_parts(tokens)\n",
    "\t\treturn tokens\n",
    "\n",
    "\tdef join_verb_parts(self, tokens):\n",
    "\t\t\"\"\"\n",
    "\t\t>>> tokenizer = WordTokenizer()\n",
    "\t\t>>> tokenizer.join_verb_parts(['خواهد', 'رفت'])\n",
    "\t\t['خواهد_رفت']\n",
    "\t\t>>> tokenizer.join_verb_parts(['رفته', 'است'])\n",
    "\t\t['رفته_است']\n",
    "\t\t>>> tokenizer.join_verb_parts(['گفته', 'شده', 'است'])\n",
    "\t\t['گفته_شده_است']\n",
    "\t\t>>> tokenizer.join_verb_parts(['گفته', 'خواهد', 'شد'])\n",
    "\t\t['گفته_خواهد_شد']\n",
    "\t\t>>> tokenizer.join_verb_parts(['خسته', 'شدید'])\n",
    "\t\t['خسته', 'شدید']\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tresult = ['']\n",
    "\t\tfor token in reversed(tokens):\n",
    "\t\t\tif token in self.before_verbs or (result[-1] in self.after_verbs and token in self.verbe):\n",
    "\t\t\t\tresult[-1] = token +'_'+ result[-1]\n",
    "\t\t\telse:\n",
    "\t\t\t\tresult.append(token)\n",
    "\t\treturn list(reversed(result[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = r\"C:\\\\Users\\\\TK_adm\\\\Documents\\\\HSE\\\\comp_ling_progr\\\\iranica\\\\collect\\\\poems\\\\\"\n",
    "punct = {'،', '«', '»', ':', '!', '؟', '؛','...', '.', '(',')'}\n",
    "\n",
    "Lemmas = {}\n",
    "\n",
    "with open(path+'corpus_big.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        words = line.split()\n",
    "        for w in words:\n",
    "            word = w\n",
    "            pre, post = '',''\n",
    "            #haypunct = True\n",
    "            tokens = []\n",
    "\n",
    "            if len(word) and word[0] in punct: \n",
    "                pre = word[0]\n",
    "                word = word[1:]\n",
    "                tokens.append(pre)\n",
    "            if len(word) and word[-1] in punct:\n",
    "                post = word[-1]\n",
    "                word = word[:-1]\n",
    "                tokens.append(post)\n",
    "            #if len(word) and word[0] not in punct and word[-1] not in punct:\n",
    "                #haypunct = False\n",
    "                #tokens.append(word)\n",
    "            tokens.append(word)\n",
    "        \n",
    "            for token in tokens:\n",
    "                if token in Lemmas:\n",
    "                    Lemmas[token] += 1\n",
    "                else:\n",
    "                    Lemmas[token] = 1\n",
    "        \n",
    "        \n",
    "with open(path+'frequency_punct1.txt', 'w', encoding='utf-8') as w:\n",
    "    dicSorted = sorted(Lemmas.items(), key=lambda tup: tup[1], reverse=True)\n",
    "    for word in dicSorted:\n",
    "        w.write(word[0]+'\\t'+str(word[1])+'\\n')\n",
    "        #i+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36894"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52612"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "with open(path+'corpus_big.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        words = line.split()\n",
    "        for w in words:\n",
    "            word = w\n",
    "            pre, post = '',''\n",
    "            haypunct = True\n",
    "            tokens = []\n",
    "            while haypunct:\n",
    "                if len(word) and word[0] in punct: \n",
    "                    pre = word[0]\n",
    "                    word = word[1:]\n",
    "                    tokens.append(pre)\n",
    "                if len(word) and word[-1] in punct:\n",
    "                    post = word[-1]\n",
    "                    word = word[:-1]\n",
    "                    tokens.append(post)\n",
    "                if len(word) and word[0] not in punct and word[-1] not in punct:\n",
    "                    haypunct = False\n",
    "                    tokens.append(word)\n",
    "                    \n",
    "                    for token in tokens:\n",
    "                        if token in Lemmas:\n",
    "                            Lemmas[token] += 1\n",
    "                        else:\n",
    "                            Lemmas[token] = 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path+'frequency_punct_alpha.txt', 'w', encoding='utf-8') as w:\n",
    "    dicSorted = sorted(Lemmas.items(), key=lambda tup: tup[0], reverse=False)\n",
    "    for word in dicSorted:\n",
    "        w.write(word[0]+'\\n')\n",
    "        #i+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Corpus = set()\n",
    "with open('BijanKhan_Distinct_Words.txt','r', encoding='utf-8') as corpus:\n",
    "    for line in corpus:\n",
    "        Corpus.add(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76707"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path+'corp_rub_corpus_dif.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in Lemmas:\n",
    "        if w in Vocab:\n",
    "            continue \n",
    "        if w in Corpus:\n",
    "            continue\n",
    "        f.write(w+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بباشد\n",
      "بدادن\n",
      "بدرودن\n",
      "بنوشتن\n",
      "بگرفتیم\n",
      "بگداختن\n",
      "بگشادن\n",
      "باختن\n",
      "ببریدن\n",
      "برفتن\n",
      "بگریختن\n",
      "بدیدن\n",
      "بگفتن\n",
      "بنبشتن\n",
      "بپرداختن\n"
     ]
    }
   ],
   "source": [
    "pre = {'ز','ب','و'}\n",
    "post = {'ست','م','ت','ش'}\n",
    "\n",
    "with open(path+'corp_rub_corpus_dif_cut.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in Lemmas:\n",
    "        if w in Vocab:\n",
    "            continue \n",
    "        if w in Corpus:\n",
    "            continue\n",
    "        word = w\n",
    "        if len(word) and word[0] in pre:\n",
    "            if len(word) and (word[1:] in Vocab or word[1:] in Corpus): \n",
    "                if Vocab[word[1:]] == 'гл.\\n':\n",
    "                    print(word)\n",
    "                else:\n",
    "                    f.write(word[0]+'\\t'+word[1:]+'\\n')\n",
    "        if len(word) and word[-1] in post:\n",
    "            if len(word) and (word[:-1] in Vocab or word[:-1] in Corpus):  \n",
    "                    f.write(word[:-1]+'\\t'+word[-1]+'\\n')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#27148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'гл.\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab['افتادن']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
